# ğŸ“˜ Feature Engineering Notes

## ğŸ§  What is Feature Engineering?
Feature engineering is the process of converting **raw data into meaningful features** that machine learning models can understand and learn from.

ğŸ‘‰ Better features = better models
ğŸ‘‰ Poor features = poor predictions ğŸ—‘ï¸


![alt text](image-1.png) ![alt text](image-2.png)

---

## 1ï¸âƒ£ Feature Transformation
Transform existing features to improve model performance.

---

### aï¸âƒ£ Missing Value Transformation ğŸ•³ï¸
Missing values are very common in real-world datasets.

#### Common techniques:
- **Mean / Median Imputation** â—
  Suitable for numerical data
- **Mode Imputation** ğŸ“Š
  Best for categorical data
- **Constant Value** (`0`, `"Unknown"`) ğŸ·ï¸
- **Drop rows or columns** âŒ
  Use only if missing values are very few
- **Advanced methods**: KNN, regression imputation ğŸ¤“

ğŸ’¡ **Tip**
Use **median** instead of mean when outliers exist.

---

### bï¸âƒ£ Handling Categorical Features ğŸ·ï¸
Machine learning models do not understand text data.

#### Encoding techniques:
- **Label Encoding** ğŸ”¢
  Example: `Low â†’ 0, Medium â†’ 1, High â†’ 2`
  âš ï¸ Use only when order matters
- **One-Hot Encoding** ğŸ¯
  Example: `Color_Red, Color_Blue, Color_Green`
- **Target Encoding** ğŸ“ˆ
  Category replaced by mean target value
- **Frequency Encoding** ğŸ“Š
  Category replaced by its frequency

ğŸ’¡ **Tip**
Tree-based models handle encodings better than linear models ğŸŒ³

---

### cï¸âƒ£ Outlier Detection ğŸš¨
Outliers are extreme values that can negatively affect models.

#### Detection methods:
- **IQR (Interquartile Range)** ğŸ“¦
- **Z-score method** ğŸ“
- **Box plots** ğŸ“Š
- **Data visualization** ğŸ‘€

#### Handling techniques:
- Remove âŒ
- Cap values (Winsorization) ğŸ©
- Log transformation ğŸ“‰

âš ï¸ Not all outliers are bad. Some contain valuable information.

---

### dï¸âƒ£ Feature Scaling ğŸ“
Scaling ensures features are on similar ranges.


![alt text](image-3.png) ![alt text](image-4.png)


#### Scaling methods:
- **Min-Max Scaling** ğŸ“
  Scales data between `0 and 1`
- **Standardization (Z-score)** ğŸ“Š
  Mean = 0, Standard Deviation = 1
- **Robust Scaling** ğŸ›¡ï¸
  Uses median and IQR (outlier-resistant)

#### Required for:
- Linear Regression
- Logistic Regression
- KNN
- SVM

#### Not required for:
- Decision Trees
- Random Forest
- XGBoost

---

## 2ï¸âƒ£ Feature Construction ğŸ§©
Creating **new features** from existing ones.

#### Examples:
- `Total_Price = Price Ã— Quantity` ğŸ’°
- `Age = Current_Year - Birth_Year` ğŸ‚
- `BMI = Weight / HeightÂ²` ğŸ§

ğŸ¯ Benefits:
- Captures hidden patterns
- Injects domain knowledge
- Often improves accuracy more than model tuning

ğŸ’¡ **Rule of thumb**
Strong features can outperform complex models ğŸ˜‰

---

## 3ï¸âƒ£ Feature Selection ğŸ¯
Select only the most important features.

#### Why?
- Faster training âš¡
- Reduced overfitting ğŸ§ 
- Better interpretability ğŸ‘“

#### Selection methods:
- **Filter methods** ğŸ§¹
  Correlation, Chi-square
- **Wrapper methods** ğŸ
  Recursive Feature Elimination (RFE)
- **Embedded methods** ğŸŒ±
  Lasso, Ridge, Tree-based feature importance

ğŸ’¡ **Tip**
Fewer meaningful features > many noisy features ğŸ†

---

## 4ï¸âƒ£ Feature Extraction ğŸ§ âœ¨
Reduce dimensionality by transforming data into a new feature space.

#### Common techniques:
- **PCA (Principal Component Analysis)** ğŸ”„
- **LDA (Linear Discriminant Analysis)** ğŸ“Š
- **Autoencoders** ğŸ¤–
- **Text features**: TF-IDF, Word Embeddings ğŸ“

ğŸ¯ Use when:
- Dataset has too many features
- Features are highly correlated
- Visualization is needed

âš ï¸ Drawback
Reduced interpretability compared to original features.

---

## ğŸ§  Final Advice from Experience ğŸ§“
- Spend **more time on features than models**
- Simple models with great features beat complex models
- Always visualize your data ğŸ‘€
- Understand the business problem ğŸ—£ï¸

---

## ğŸ“Œ TL;DR Stickers ğŸ˜„
ğŸ§¹ Clean your data
ğŸ§© Build smart features
ğŸ¯ Select important ones
ğŸ§  Extract when necessary
ğŸš€ Then train models

---

